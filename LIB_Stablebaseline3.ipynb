{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF21cOz17bPL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install -U \"ray[rllib]\"\n",
        "!pip install tianshou\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
        "!pip install mat4py\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V3XZgWVE5nE5"
      },
      "source": [
        "# LIBPackEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "aw0t5NkH5m4k",
        "outputId": "b098db97-c213-428f-c99f-b281012a972b"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from LIBEnv.LIB_gym_delta import LIBPackEnv\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "env = LIBPackEnv('./LIBEnv/PANmodel.mat', number_of_cells=6, T=25, dt=1)\n",
        "\n",
        "pack_res=[]\n",
        "rewards = []\n",
        "pack_res.append(env.reset()[0].reshape(3,6))\n",
        "done = False\n",
        "while not done:\n",
        "    next_state, reward, done, _, info = env.step(np.ones(7))\n",
        "    pack_res.append(next_state.reshape(3,6))\n",
        "    rewards.append(reward)\n",
        "\n",
        "pack_res = np.array(pack_res)\n",
        "plt.figure(figsize=(18,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(pack_res[:,0,:])\n",
        "plt.xlabel('time (s)')\n",
        "plt.ylabel('pack SOC')\n",
        "plt.grid('on')\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(pack_res[:,1,:])\n",
        "plt.xlabel('time (s)')\n",
        "plt.ylabel('pack voltage (V)')\n",
        "plt.grid('on')\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(pack_res[:,2,:])\n",
        "plt.xlabel('time (s)')\n",
        "plt.ylabel('pack core temp (Â°C)')\n",
        "plt.grid('on')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tensorboard "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Tensorboard - click the refresh button once training is running\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0DcQa_RX5lF1"
      },
      "source": [
        "# stable baseline3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq:\n",
        "    :param log_dir: Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "       pass\n",
        "        # Create folder if needed\n",
        "        # if self.save_path is not None:\n",
        "        #     os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose >= 1:\n",
        "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose >= 1:\n",
        "                    print(f\"Saving new best model to {self.save_path}\")\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VgCQlkUqtOa-",
        "outputId": "36705df8-d277-4599-e999-994af5dfeac6"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, DDPG, A2C, TD3, SAC\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "import os \n",
        "\n",
        "timesteps = 1e7\n",
        "log_dir = \"./tmp/\"\n",
        "\n",
        "check_env(env, warn=True)\n",
        "env = Monitor(env, log_dir+'monitor.csv', override_existing=False)\n",
        "model = SAC(\"MlpPolicy\", env, verbose=1, \n",
        "            tensorboard_log=\"logs\", \n",
        "            learning_rate=1e-3, \n",
        "            buffer_size=int(1e6),\n",
        "            batch_size=1024,\n",
        "            gamma=0.95,\n",
        "            tau=0.05,\n",
        "            policy_kwargs=dict(net_arch=[512, 512, 512]))\n",
        "\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=2e4, log_dir=log_dir)\n",
        "\n",
        "# Train the agent\n",
        "# model = model.load('best_model.zip', env)\n",
        "model.learn(total_timesteps=timesteps, callback=callback, progress_bar=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.load('./best_model', env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "results = pd.read_csv('./monitor.csv').reset_index()\n",
        "results.columns = [results.iloc[0]]\n",
        "results.drop([0], inplace=True)\n",
        "returns = np.array(results['r'].astype(float)).ravel()\n",
        "\n",
        "window = 100\n",
        "rolling_mean = pd.Series(returns).rolling(window).mean()\n",
        "std = pd.Series(returns).rolling(window).std()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(returns)\n",
        "plt.plot(rolling_mean, color='red', linewidth=2.5)\n",
        "plt.fill_between(range(len(rolling_mean)),rolling_mean-std, rolling_mean+std, color='violet', alpha=0.5)\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Return')\n",
        "plt.grid('on')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "results = pd.read_csv('./monitor.csv').reset_index()\n",
        "results.columns = [results.iloc[0]]\n",
        "results.drop([0], inplace=True)\n",
        "returns = np.array(results['l'].astype(float)).ravel()\n",
        "\n",
        "window = 100\n",
        "rolling_mean = pd.Series(returns).rolling(window).mean()\n",
        "std = pd.Series(returns).rolling(window).std()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(returns)\n",
        "plt.plot(rolling_mean, color='red', linewidth=2.5)\n",
        "plt.fill_between(range(len(rolling_mean)),rolling_mean-std, rolling_mean+std, color='violet', alpha=0.5)\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Episode Length / Charge Duration (s)')\n",
        "plt.grid('on')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "X59u3VZ2KaZk",
        "outputId": "177053e0-a1a2-4e8d-f1bd-3d37f6515fa8"
      },
      "outputs": [],
      "source": [
        "env = LIBPackEnv('./LIBEnv/PANmodel.mat', number_of_cells=6, T=25, dt=1, use_priority=True)\n",
        "\n",
        "MPC_HORIZON = 8\n",
        "MPC_controller = MPC_Controller(MPC_HORIZON)\n",
        "\n",
        "pack_res=[]\n",
        "rewards = []\n",
        "actions = []\n",
        "\n",
        "done = False\n",
        "pack_res.append(env.reset()[0].reshape(3,6))\n",
        "while not done:\n",
        "    MPC_action = MPC_controller.optimize(env)\n",
        "    MPC_action = np.hstack([MPC_action,1])\n",
        "    next_state, reward, done, _, info = env.step(MPC_action)\n",
        "    pack_res.append(next_state.reshape(3,6))\n",
        "    rewards.append(reward)\n",
        "    actions.append(action)\n",
        "\n",
        "pack_res = np.array(pack_res)\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(pack_res[:-1,0,:])\n",
        "plt.ylabel('pack SOC')\n",
        "plt.grid('on')\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(pack_res[:-1,1,:])\n",
        "plt.ylabel('pack voltage')\n",
        "plt.grid('on')\n",
        "plt.subplot(2,2,3)\n",
        "plt.plot(pack_res[:-1,2,:])\n",
        "plt.ylabel('pack core temp')\n",
        "plt.grid('on')\n",
        "plt.subplot(2,2,4)\n",
        "plt.plot(np.array(actions)[:-1].reshape(-1,6+1)[:,:-1])\n",
        "plt.plot(np.array(actions)[:-1].reshape(-1,6+1)[:,-1], \"--\", color='red')\n",
        "plt.ylabel('current')\n",
        "plt.grid('on')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.array(actions)[:-1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(np.abs(pack_res[:-1,0,:] - np.mean(pack_res[:-1,0,:], axis=-1).reshape(-1,1)).sum(-1))\n",
        "plt.xlabel('time')\n",
        "plt.ylabel('Balance Cost')\n",
        "plt.grid('on')\n",
        "plt.ylim([0,0.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(pack_res[:-1,0,:].max(-1) - pack_res[:-1,0,:].min(-1))\n",
        "plt.xlabel('time')\n",
        "plt.ylabel('SOC dev')\n",
        "plt.grid('on')\n",
        "plt.ylim([0,0.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.abs(pack_res[:-1,0,:] - np.mean(pack_res[:-1,0,:], axis=-1).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.mean(pack_res[:-1,0,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pack_res[-2,0,:], np.std(pack_res[-2,0,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('./last_model.pth')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6oLYPqkn-gP8"
      },
      "source": [
        "# Tianshu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeAJGEMP-n3v",
        "outputId": "c7450abd-ec79-4abf-ce77-d6cfec762ed1"
      },
      "outputs": [],
      "source": [
        "pip install tianshou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EmT7PN-9ldL",
        "outputId": "a6a5c809-381d-4f56-e875-e9181841f91f"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch, numpy as np, torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import tianshou as ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GskHo6R7-6-X"
      },
      "outputs": [],
      "source": [
        "lr, epoch, batch_size = 1e-3, 10, 64\n",
        "train_num, test_num = 10, 100\n",
        "gamma, n_step, target_freq = 0.9, 3, 320\n",
        "buffer_size = 20000\n",
        "eps_train, eps_test = 0.1, 0.05\n",
        "step_per_epoch, step_per_collect = 10000, 10\n",
        "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))  # TensorBoard is supported!\n",
        "# For other loggers: https://tianshou.readthedocs.io/en/master/tutorials/logger.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
